---
title: "7302c CUTe"
author: "Amar Rao"
date: "December 14, 2017"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
    theme: united
    highlight: tang
    fig_width: 7
    fig_height: 6
    fig_caption: true
    code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

#clear all environment and session variables
rm(list = ls(all = TRUE))


```


Libraries
```{r}

library(knitr)
library(tidyverse)
library(lubridate)
library(caret)
library(DMwR)
library(forecast)
library(lubridate)
library(imputeTS)
library(TTR)
library(graphics)
library(zoo)
```

## Reading data

* set working directory
```{r}
setwd('C:/Users/212629693/Documents/Personal/ds-classes/INSOFE_Batch35_7302c_CUTe_Team17')

```

### Load the data set

```{r}
fin_data_orig <- read.csv('Datasets/train_data.csv', header = TRUE, sep = ',', na.strings = "", colClasses = "double", numerals = "no.loss")
str(fin_data_orig, list.len = ncol(fin_data_orig))
head(fin_data_orig)
```

* We know y2 is a binary column. converting that to factor.

```{r}
cat_attrs <- c('y2')
num_attrs <- setdiff(colnames(fin_data_orig), cat_attrs)

fin_data_orig[cat_attrs] <- data.frame(sapply(fin_data_orig[cat_attrs], as.factor))
str(fin_data_orig, list.len = ncol(fin_data_orig))
```

```{r}
summary(fin_data_orig)
```


###Observations:

* There're 109 regressors and 2 target variables (y1 and y2) and 1769 observations
* Data appears to already have been scaled and standardized. 
* There are a few empty columns (ie all rows are blank in these columns )
* Some values are missing in other columns
* Timestamp starts as a day number at 14 and goes up to 1782 days (1769 days in total)



## Preprocessing
### Empty Columns

  * Following columns are completely blank (column with all NA values) in the data frame.

```{r}
emptycols <- colnames(fin_data_orig[,sapply(fin_data_orig, function(x) { all(is.na(x))})])
emptycols
```

*Removing these columns

```{r}
fin_data <- fin_data_orig[, sapply(fin_data_orig, function(x) { !all(is.na(x))})]
```

### Handle NAs

* Checking how spread the NAs are in other columns
```{r}
colswithna <- colSums(is.na(fin_data))

sort(colswithna, decreasing = TRUE)

```

#### The following columns have very large number of NAs so imputing those would make the data diluted.

*f_63
*f_27
*f_3
*f_28
*f_31
*f_35
*f_25
*f_17

####So removing those columns. Will use knnImputation for the rest once I do the test-train split.

```{r}
fin_data <- subset(fin_data,select = -c(f_63, f_27, f_3, f_28, f_31, f_35, f_25, f_17))

```

### convert timestamp to date
* The timestamps are given as day numbers starting with 14. We have observations for 1769 days (approx 4.8 years of data)

* Generating dates from 2012 and setting that as the id columns
```{r}
origindate <- as.Date('2011-12-31', format = '%Y-%m-%d')
origindate
fin_data$timestamp = as.Date(fin_data$timestamp, origindate)
rownames(fin_data) <- fin_data$timestamp

#removing timestamp column
fin_data$timestamp <- NULL
head(fin_data)
tail(fin_data)
```

* checking if there're any sparse columns (ie columns with very large number of zeros)

```{r}
sort(colSums(fin_data == 0), decreasing = TRUE)

```

* from the above we can see that t_10, t_13, t_30, t_20 all have very large number of 0s. Checking to see if these are factors

```{r}
tail(fin_data$t_10, 200)
tail(fin_data$t_13, 200)
tail(fin_data$t_30, 200)
tail(fin_data$t_20, 100)
```

* from looking at the data in these tables, it is clear that these are not factors.
* since these have very large number of zeros, they cannot have any meaningful impact on the target variables. So removing them.

```{r}
fin_data <- subset(fin_data, select = -c(t_10, t_13, t_30, t_20))


```


### Train-test split

```{r}
#Since this is chronological data, we cannot do random sampling.
# Setting the first 70% to train and last 30% to test
train_rows <- 1:round(nrow(fin_data)*0.7)

fin_train <- fin_data[train_rows, ]

fin_test <- fin_data[-train_rows, ]

```

###Imputation

```{r}

library(RANN)
set.seed(1234)
preproc_preds <- preProcess(x = subset(fin_train, select = -c(y1, y2)), method = c("knnImpute"))

fin_train <- predict(object = preproc_preds, fin_train)
fin_test <- predict(object = preproc_preds, fin_test)
sum(is.na(fin_train))
sum(is.na(fin_test))

```

## Assessing fit for Timeseries Forecasting
* Need to check if y1 exhibits strong trend and some seasonality (optionally). If so, we can proceed with timeseries forecasting.



```{r}
y1ts <- ts(fin_train$y1, start = c(2012, 1,1), frequency = 365.25)
plot(y1ts,
     type="l",
     lwd=2,
     col="blue",
     xlab="Daily",
     ylab="Change",
     main="Time series plot for stock - for target variable y1")
```

```{r}
y1decomp <- decompose(x = y1ts)
plot(y1decomp)
```

###Observations:
* We see that there's a fairly steady trend moving within a small range
* Seasonality at daily frequency seem to exist. 

###Will first use this without any transformations to see if ARIMA model exists
### Arima requires xargs to be of rank = ncols, so we need to ensure that's the case

```{r}
library(Matrix)
fin_x_reg <- subset(fin_train, select = -c(y1, y2))

nrow(fin_x_reg)
ncol(fin_x_reg)
rankMatrix(fin_x_reg)
```


* Rank of the matrix (87) is less than the number of columns. This will cause errors running Arima or auto.arima. So need to eliminate that.

```{r}

constant_cols <- names(fin_train[, sapply(fin_train, function(v) var(v, na.rm=TRUE)==0)])
constant_cols
str(constant_cols)
```

* Will exclude these columns for further analysis

```{r}
fin_train <- fin_train[, !colnames(fin_train) %in% constant_cols]
fin_test <- fin_test[, !colnames(fin_test) %in% constant_cols]


#confirming if that fixes the rank issue
rankMatrix(as.matrix(subset(fin_train, select = -c(y1, y2))))
```

* now creating separate dataframes for y1 and y2

```{r}

y1_train <- subset(fin_train, select = -y2)
y1_test <- subset(fin_test, select = -y2)
y2_train <- subset(fin_train, select = -y1)
y2_test <- subset(fin_test, select = -y1)


```


##MODEL BUILDING

##ARIMA with no transformation 
*ACF

```{r}

acf(y1ts)


```

ACF shows that there's 4 significant lags so we can use non-seasonal q = 4

*PACF

```{r}
pacf(y1ts)

```

PACF indicates 5 significant lags, so we can use non seasonal p = 5

* Check if we need to difference

```{r}
ndiffs(y1ts)
nsdiffs(y1ts)
```

* No need to difference

###First we will build a basic Arima(5,0,4) model without any xregs
* Premise here is that previous values of y1 are strongest predictors for future y1

```{r}
arima1 <- Arima(y = y1ts, order = c(5,0,4))
arima1
```

*Predictions with arima1

```{r}
arima1preds <- forecast(object = arima1, h = nrow(y1_test))

plot(arima1preds)
```


* Performance Metrics for arima1

```{r}
accuracy(arima1preds, y1_test$y1)

```

*Getting a MAPE of 154.59 with training set and 140.37 with test set.

##Now Arima with xregs (all columns)

```{r}
arima2 <- Arima(y = y1ts, order= c(5,0,4), xreg = subset(y1_train, select = -y1))
```

* Getting the error shown above.

###So Arima with all xregs in the dataset doesn't work. We will have to find out which columns to include.
For that, using StepAIC. IN that process, we will also evaluate the performance of OLS.

##StepAIC
###First with OLS
```{r}


base_linreg <- lm(y1 ~ ., data = y1_train)
base_linreg
```

###Predictions with OLS and its performance

```{r}

ols_all_preds <- predict(base_linreg, y1_test)

print('Error metrics for Train data')
print(regr.eval(ols_all_preds, y1_train$y1))
print("")
print('Error metrics for Test data')
print(regr.eval(ols_all_preds, y1_test$y1))
```

###OLS with all variables is giving a MAPE of 2.34 with Training data and 0.99 with Test Data!!!

* Let's see if StepAIC can do better

```{r}
library(MASS)
aicoptions <- stepAIC(base_linreg, direction = "both")
```


####Based on stepAIC, will use only the following regressors:

lm(formula = y1 ~ d_0 + d_1 + d_4 + f_16 + f_18 + f_20 + f_21 + 
    f_29 + f_34 + f_39 + f_40 + f_48 + f_49 + f_52 + f_58 + t_2 + 
    t_6 + t_24 + t_25 + t_34 + t_39 + t_43 + t_44 + f_33 + f_9 + 
    f_19 + t_14, data = y1_train)
    
    
```{r}
colnames(aicoptions$model)

myfactors <- colnames(aicoptions$model)
myfactors
myfactors <- myfactors[-1]
myfactors

#reformulate(termlabels = listoffactors, response = 'y')
lmformula <- reformulate(termlabels = myfactors, response = "y1")
lmformula
aic_rec_mdl <- lm(formula = lmformula, data = y1_train)


aic_rec_mdl
```

```{r}
aic_preds <- predict(aic_rec_mdl, y1_test)

print('Error metrics for Train data')
print(regr.eval(aic_preds, y1_train$y1))
print("")
print('Error metrics for Test data')
print(regr.eval(aic_preds, y1_test$y1))



```

* WIth StepAIC recommended model MAPE on training went up to 3.2 and on test went up to 1.09


##Now we will try Arima with regressors based on StepAIC recommended regressors

```{r}

new_y1_xreg <- subset(y1_train[, colnames(aic_rec_mdl$model)], select = -y1)

arima3 <- Arima(y = y1ts, order = c(5,0,4), xreg = new_y1_xreg)

arima3_preds <- forecast(object = arima3, h = nrow(y1_test), xreg = new_y1_xreg)
print(accuracy(arima3_preds,y1_test$y1))

plot(arima3_preds)


```

* Looks like regressors have made the predictions worse. MAPE is 1064.665 for training set and 354.393 for test set.


ACF and PACF of residuals

```{r}
acf(arima3$residuals)
```
```{r}
pacf(arima3$residuals)
```

#Trying with auto.arima

```{r}

arima4 <- auto.arima(y = y1ts, xreg = new_y1_xreg)


arima4

arima4_preds <- forecast(object = arima4, h = nrow(y1_test), xreg = new_y1_xreg)
print(accuracy(arima4_preds,y1_test$y1))

plot(arima4_preds)

```

Not much improvement in MAPE with AutoArima suggested (3,0,0)

##Seeing the performance of LASSO

###First will try a LASSO model with all variables

```{r}
library(glmnet)
set.seed(1234)

cv_lasso <- cv.glmnet(as.matrix(subset(y1_train, select = -y1)), as.matrix(y1_train$y1), alpha = 1, type.measure = "mae", nfolds = 4)
par(mfrow=c(1,2))
plot(cv_lasso)
plot(cv_lasso$glmnet.fit, xvar = 'lambda', label = TRUE)
par(mfrow=c(1,1))


```

* Now using the min lambda, running lasso regression

```{r}
lasso_1 <- glmnet(x = as.matrix(subset(y1_train, select = -y1)), y = as.matrix(y1_train$y1), alpha = 1, lambda = cv_lasso$lambda.min)

lasso_1_preds <- predict(lasso_1, as.matrix(subset(y1_test, select = -y1)))
regr.eval(lasso_1_preds, y1_test$y1)


```

* This gives a MAPE of 8.85% on test data

###Now trying LASSO with the set of variables from StepAIC

```{r}
new_y1_xreg
X_train = as.matrix(new_y1_xreg)

X_test <- as.matrix(subset(y1_test[, colnames(aic_rec_mdl$model)], select = -y1))

cv_lasso_1 <- cv.glmnet(X_train, as.matrix(y1_train$y1), alpha = 1, type.measure = "mse", nfolds = 4)

par(mfrow=c(1,2))
plot(cv_lasso_1)
plot(cv_lasso_1$glmnet.fit, xvar = 'lambda', label = TRUE)
par(mfrow=c(1,2))



```



```{r}
lasso_2 <- glmnet(X_train, as.matrix(y1_train$y1), lambda = cv_lasso_1$lambda.min, alpha = 1)

lasso_2_preds <- predict(lasso_2, X_test)

regr.eval(lasso_2_preds, y1_test$y1)
```

MAPE with this LASSO model gives a mape of 1.10


###refactor below

* The target variable y1 is a percent change. Will use another column to get a price so that
* it is easier to see trend and forecast



```{r}

setdailyprice <- function(daily_price) {
  retprice <- daily_price
  for(i in 1:length(daily_price)) {
    if (i == 1) {
      retprice[i] = 1000
    } else {
      retprice[i] = retprice[i-1]*(1+ daily_price[i])
    }

  }
  return(retprice)
}

fin_data$DailyPrice <- setdailyprice(fin_data$y1)
head(fin_data)


```




* To see if there's a seasonality at all, taking weekly average first (then monthly average)
* and decomposing.

```{r}
fin_train$Year <- as.numeric(year(rownames(fin_train)))
fin_train$Month <- as.numeric(month(rownames(fin_train)))
fin_train$WeekOfYear <- isoweek(rownames(fin_train))

fin_train_weekly <- fin_train %>% group_by(Year, Month, WeekOfYear) %>% summarize("WeeklyAvgY1" = mean(y1))
fin_train_monthly <- fin_train %>% group_by(Year, Month) %>% summarize("MonthlyAvgY1" = mean(y1))

```


```{r}


weeklyy1ts <- ts(fin_train_weekly$WeeklyAvgY1, start = c(2012, 1,1), frequency = 52)
plot(decompose(weeklyy1ts))
```


```{r}
monthlyy1ts <- ts(fin_train_monthly$MonthlyAvgY1, start = c(2012, 1,1), frequency = 12)
plot(decompose(monthlyy1ts))
```

### Observations:
Based on the above, it is clear that there's no clear seasonality that affects the daily percent change in the value of y1. Consequently will not do a Seasonal TS forecasting

```{r}
#Cleaning up Year, Month, and WeekOfYear column as we won't need that.

fin_train$Year = NULL
fin_train$Month = NULL
fin_train$WeekOfYear = NULL

```





## Initial Model
* For this, we need to first create a matrix of all regressors

* Trying StepAIC to identify significant regressors




#### Assessing the colinearity

```{r}
library(car)
vif(mod = base_linreg)

library('corrplot')


corrplot(cor(subset(y1_train, select= c(d_0, d_1, d_2, d_3, d_4, f_0, f_5, f_7, f_8, f_9, f_11, f_13, f_14, f_15, f_16, f_18,  f_19, f_20 , f_22 , f_23 , f_24 , f_29 , f_30 , f_32 , f_33 , f_34 , f_36 , f_37 , f_39 , f_40 , f_41 , f_42 , f_43 , f_44 , f_46 , f_48 , f_49 , f_50 , f_51 , f_52 , f_53 , f_54 , f_56 , f_58 , f_59 , f_60 , f_62 , t_0 , t_2 , t_3 , t_6 , t_7 , t_11 , t_12 , t_14 , t_17 , t_18 , t_19 , t_21 , t_24 , t_25 , t_27 , t_29 , t_32 , t_33 , t_34 , t_35 , t_36 , t_37 , t_38 , t_39 , t_40 , t_43 , t_44)), use = "complete.obs"), method = "number")

```


## Final Model
## Predictions
## Performance

